---
title: "METARET -- a Meta Analysis of the External Validity of Risk Elicitation Tasks"
author: "Paolo Crosetto"
date: "This version: June 2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F)
```

This repository hosts all the development files of the METARET project on the **external validity** of **risk elicitation tasks** (RETs) -- i.e. their ability to predict self-reported or real-world behavior.

## Aim of the project

The accurate measure of **risk preferences** is of large importance in both theoretical and applied work. Despite this importance, increasing experimental evidence points to the fact that the behavioral measures we use to elicit risk attitudes have **low external validity**. 

Lottery-based risk elicitation tasks (*RETs*), at least in the way they are typically implemented in behavioral economics and psychology, 1. **correlate poorly** with self-reported risk attitudes, real-world risk behaviors, and among themselves; 2. introduce distinct **measurement errors** and behavioral biases, and 3. are not robust to **test-retest** exercises. 

But the issue can not be settled by a handful of papers, that despite all efforts are limited in sample size, and types of task and questionnaires considered. 

The good news is that the **data** to assess the external validity of a large variety of RETs and a large variety of self-reported questionnaires **already exists**. It sits in the drawers of dozens of experimentlasits that have used RETs for their disparate aims. If collected, the *existing data* would give us a detailed, precise, and extensive **map of the external validity** of RETs under a variety of experimental conditions.

The aim of METARET -- the Meta-analysis of the external validity of RETs --  is to collect in a single, public, open repository all the data pertaining to the external validity of RETs. Its objectives are to

- **document exhaustively** the scope and limits of the external validity of different RETs.
- provide a **detailed map** of which **features** of a RET are more conducive to external validity validity.
- serve as a basis for the **development** of a more predictive lottery-based RET (*French ANR grant under submission*).

METARET will collect data of papers that have run:

- any **lottery-based RET** (incentivized or not)
- any **other measure of risk** attitudes (questionnaire, bids in an auction, self-reported, implied by behavior in another game (insurance, trust...))
- any **self-declared real-world risky behaviour**.

The metric of interest will be (linear and rank) correlations between any two RETs, any RET and any questionnaire, any RET and any real-world behavior. For lottery-based RETs, two metrics will be used: bare choices -- whose domain and granularity change across tasks -- and implied CRRA coefficients of risk attitudes *r* -- that are burdened by theoretical assumptions but are more comparable across tasks. 

The aim is to be as large as possible, to allow the map to cover most possible ground -- data availability will impose us to restric attention to certain broad class of RETs and of self-reported measures in due time. 


## Resources

The project can count on:

- a **web site** [(here)]() setting up the aim of the project, tracking its ongoing devlopment of the project, and giving information on how to contribute.
- a **shiny interactive app** [(here)](https://paolocrosetto.shinyapps.io/METARET/) where the data accumulated so far can be explored and visualised
- an **OSF page** [(here)](https://osf.io/h2z56/) where the project has been pre-registered
- this **github repository** where the real work is done and updated, and that contains
  - **data** from each of several *contributed papers* by experimental economists and social psychologists. 
  - **code** to format each original dataset into a common format for later analysis
  - **results** in the form of plots and tables (*work in progress*)
  - **code** for the interactive shiny app (*work in progress*)


## How to contribute

HERE HOW TO CONTRIBUTE

- git pull request + you add your own code. See the API
- send me your data: it will be treated in such and such a way
- send me your summary stats
- other 

## Contributed papers (list updates as papers are contributed)

- Crosetto, Paolo, and Filippin, Antonio, *The Bomb Risk Elicitation Task*, JRU, 2013 [paper](https://link.springer.com/article/10.1007/s11166-013-9170-z) [data](/Data/Crosetto_Filippin_Experimental_Economics_2016)
- Crosetto, Paolo, and Filippin, Antonio, *A Theoretical and Experimental Appraisal of Four Risk ELicitation Methods*, ExEc, 2016 [paper](https://link.springer.com/article/10.1007/s10683-015-9457-9) [data](/Data/Crosetto_Filippin_Journal_Risk_Uncertainty_2013/)

## First look at the results

As a first step, I compute the (Pearson) correlation of each RET to (each of a series of) self-reported risk measures.

Each task is represented by a point estimate + confidence interval. Here are the results of correlations between several tasks and:

- the SOEP risk question 
- the DOSPERT scale and its subscales

The plots and analyses are updated for each new contributed paper.

```{r, fig.height=12, fig.width = 9, dpi=600}

### playing around with the correlation plots
library(tidyverse)
library(broom)
library(hrbrthemes)

theme_set(theme_ipsum_rc()+
            theme(legend.position = "bottom")+
            theme(strip.text.x = element_text(hjust = 0.5))+
            theme(panel.spacing.x = unit(0.2, "lines")))

## getting data from all the "formatted_dataset.csv" files in each subdirectory of /Data
df <- list.files(recursive = T, pattern = "^formatted_dataset") %>% 
  map_dfr(read_csv)

#### playground for the final plot -- commented away

## TODO/ need to automate this in a function, adn don't know how. MESSY!
compute_corr <- function(data, var, name) {
  data %>%
  nest(-paper, -task, -treatment) %>%
  mutate(
    test = map(data, ~ cor.test(.x$choice, .x[[var]], method = "pearson", conf.level = 0.95)), 
    tidied = map(test, tidy)
  ) %>%
  unnest(tidied, .drop = TRUE) %>% 
    mutate(questionnaire = name)
}


# SOEP
soep <- compute_corr(df, "soep", "SOEP")

# DOSPERT
dospert <- compute_corr(df, "doall", "DOSPERT")

# DOSPERT-GAMBLE
dogamble <- compute_corr(df, "dogamble", "D-gamble")

# DOSPERT-INVESTMENT
doinvest <- compute_corr(df, "doinvest", "D-invest")

# DOSPERT-HEALTH
dohealth <- compute_corr(df, "dohealth", "D-health")

# rbind
corr <- rbind(soep, dospert, dogamble, doinvest, dohealth) %>% 
  mutate(questionnaire = fct_relevel(as_factor(questionnaire), "SOEP", "DOSPERT"))

# join  the N of observations oer study
corr <- df %>% group_by(paper, task, treatment) %>% summarise(N = n()) %>% left_join(corr, by = c("paper", "task", "treatment"))

# another stupid hack
corr <- corr %>% mutate(treatment = if_else(is.na(treatment), "", treatment))

## plot on non-transformed values
corr %>%
  filter(treatment != "repeated") %>%
  mutate(p.value = cut(p.value,
                       breaks = c(-1, 0.01, 0.05, 0.1, 1),
                       labels = c("<1%", "<5%", "<10%", "n.s."))) %>%
  mutate(treatment = paste(treatment, " (N = ", N, ")", sep ="")) %>%
  ggplot(aes(reorder(treatment, estimate), estimate, color= p.value))+
  geom_errorbar(aes(ymin = conf.low , ymax = conf.high), width = 0.2)+
   geom_point(size = 2)+
   coord_flip()+
   geom_hline(yintercept = 0, linetype = "dotted", color = "indianred")+
   scale_color_manual(name = "significance", values = c("red", "orange", "yellow", "black"), drop = F)+
  scale_y_continuous(breaks = c(-0.2, 0, 0.2, 0.4), labels = c("-0.2","0","0.2","0.4"))+
   facet_grid(task~questionnaire, scales = "free_y", space = "free_y")+
   xlab("")+ylab("Pearson correlation")

```

